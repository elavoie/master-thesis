\chapter{Performance}
\label{chap:Performance}

The last section introduced techniques to reduce the run-time overhead of the
object model. We now evaluate the actual run-time and memory-usage overhead
incured by the additional flexibility. We will see that it compares favorably
to the performance of a state-of-the-art interpreter, making the approach
viable for instrumentation. Additionnally, the actual figures obtained will
allow the reader to evaluate if this approach could be applied to other usage
scenarios.

The next section explains the methodology used to evaluate the performance of
the system. We then present the baseline performance (without instrumentation)
obtained on different systems for common benchmarks. We then see the
performance impact of a chosen instrumentation and we conclude with an
interpretation of the results. 


\section{Methodology}

We compare three different systems:
\begin{itemize}
    \item Photon running over V8 (V8's full optimizations)
    \item V8 (full optimizations)
    \item SpiderMonkey interpreter (JIT disabled)
\end{itemize}

V8 was chosen to host Photon because preliminary tests showed the system to be
faster on it. The additional speed is attributed to the ability of the runtime
to do function inlining, On-Stack-Replacement and the presence of fast Garbage
Collectors. Other VMs are catching up on features so we anticipate that in the
near future they could probably be used interchangeably. We compare Photon to
V8 to quantify the performance cost incured by our approach. This really gives
the cost of harnessing some performance to provide more flexibility to the
system.  We finally compare to a popular state-of-the-art interpreter to argue
that our approach can be used wherever a manual instrumentation of that
interpreter could have been performed. 

In choosing the systems to compare, we eliminated a few current alternatives.
We assume that when faced with the task of instrumenting production code to
obtain run-time data, manually instrumenting a JIT-compiler would be deemed too
complex to be cost-effective in terms of development time. At the time of
writing, a new interpreter became available, namely JavaScriptCore's
low-level interpreter.  However, we argue that the only real instrumentation
alternative right now would be SpiderMonkey's interpreter because the
JavaScriptCore low-level interpreter is implemented in an assembly dialect to
obtain performance gains. As this new interpreter matures, we speculate that
its complexity will increase, negating most of the simplicity usually
attributed to interpreters.~\footnote{Should the reader still consider the
alternative of instrumenting JavaScriptCore's low-level interpreter instead,
she should know that in our tests, on V8's benchmarks, JavaScriptCore low-level
interpreter was roughly three times faster than SpiderMonkey's interpreter.}
Finally, we do not show performance results for Narcissus, Mozilla's JavaScript
in JavaScript interpreter, because the latest version would not run either
SunSpider or V8 benchmarks.

To assess performance, we use the SunSpider and V8 benchmark suites, since they
became the \textit{de facto} standard to compare JS VM performance. They both
come with their own different evaluation methodology. We used the methodology
of each benchmark suite without modification to make our results comparable to
other results obtained elsewhere for other systems. All the benchmarks were run
unmodified. However, the date benchmarks from SunSpider were omitted because
they relied on using \kw{eval} to access local variables, which is not
supported by our current compiler.  Since Photon's local environment use the
underlying VM's local environment (it is not reified), we believe that
supporting this feature in the future will not affect significatively the
performance results presented here.

We focus on two metrics, total running time and the maximum heap size to
respectively measure run-time performance and memory usage. Running time is
measured either directly, in the case of SunSpider's benchmarks or indirectly
through a score, in the case of V8's benchmarks. Memory usage is only used to
estimate the overhead of Photon compared to V8.

We present results in two different groups, the baseline performance and the
instrumented performance.  The baseline performance is used to measure the
minimal overhead of the approach.  This is important to assess the viability of
the approach because a baseline overhead that would be too high could make the
approach unusable in the majority of cases, independently of the other
characteristics of the system. The instrumented performance is used to measure
the impact of instrumentation on the baseline performance. It is common
knowledge that instrumenting an interpreter has little impact over its overall
performance (this was verified by Richards and al. when they instrumented
JavaScriptCore~\cite{behavior_js}). However, our approach might be more
sensible to instrumentation. We therefore show the impact of a basic
instrumentation on the performance of Photon. We used the V8 benchmarks because
they perform more and more varied operations in a single benchmark than the
SunSpider benchmarks. We count the number of occurrence at run time of each
reifed object-model operation. We chose this particular instrumentation because
it is simple, it covers every object model operation and it can be used to
reproduce the object read, write or delete proportion figure
from~\cite{behavior_js}.

% TODO: Provide machine specs and versions for both d8 and spidermonkey

% TODO ?? Provide graphic with confidence intervals and means
\section{Baseline Performance}

\subsection{Running Times}

\subsubsection{V8 benchmarks}

\input{/Users/erick/Recherche/photon-js/results/baseline/v8/time/table.tex}

\subsubsection{SunSpider benchmarks}

\input{/Users/erick/Recherche/photon-js/results/baseline/sunspider/time/table.tex}

\subsection{Memory Usage}

\subsubsection{V8 benchmarks}

\input{/Users/erick/Recherche/photon-js/results/baseline/v8/memory/table.tex}

\subsubsection{SunSpider benchmarks}

\input{/Users/erick/Recherche/photon-js/results/baseline/sunspider/memory/table.tex}

\section{Instrumented Performance}

\subsection{Running Times}

\subsubsection{V8 benchmarks}

\input{/Users/erick/Recherche/photon-js/results/instrumented/v8/time/table.tex}

\subsubsection{SunSpider benchmarks}

\input{/Users/erick/Recherche/photon-js/results/instrumented/sunspider/time/table.tex}

\subsection{Memory Usage}

\subsubsection{V8 benchmarks}

\input{/Users/erick/Recherche/photon-js/results/instrumented/v8/memory/table.tex}

\subsubsection{SunSpider benchmarks}

\input{/Users/erick/Recherche/photon-js/results/instrumented/sunspider/memory/table.tex}


\section{Interpretation}

